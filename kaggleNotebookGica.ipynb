{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# main imports \n\nimport os\nimport pandas as pd\nfrom PIL import Image\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom time import sleep\nfrom tqdm import tqdm\nimport pickle\n\nfrom tqdm import tqdm\nimport seaborn as sns\n\nimport tensorflow.keras as keras\nfrom tensorflow.keras import datasets, layers, models\nfrom tensorflow.keras.layers import Input\nfrom tensorflow.keras.models import Sequential      #This allows appending layers to existing models\nfrom tensorflow.keras.layers import Dense           #This allows defining the characteristics of a particular layer\nfrom tensorflow.keras import optimizers             #This allows using whichever optimiser we want (sgd,adam,RMSprop)\nfrom tensorflow.keras import regularizers           #This allows using whichever regularizer we want (l1,l2,l1_l2)\nfrom tensorflow.keras.utils import to_categorical   #This allows using categorical cross entropy as the cost function\n#from tensorflow.keras import Conv2D\n#from tensorflow.keras import MaxPooling2D\n#from tensorflow.keras import Flatten\n\nfrom sklearn.model_selection import train_test_split\nfrom numpy import asarray","metadata":{"execution":{"iopub.status.busy":"2022-02-02T16:37:30.735605Z","iopub.execute_input":"2022-02-02T16:37:30.736163Z","iopub.status.idle":"2022-02-02T16:37:30.743394Z","shell.execute_reply.started":"2022-02-02T16:37:30.736126Z","shell.execute_reply":"2022-02-02T16:37:30.742472Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"#main functions\n\ndef get_path(Id,df):\n    \"\"\"Input: row number of the dataframe\n       Returns the filepath for the corresponding image\"\"\"\n    \n    #get path from dataframe\n    path2 = df.loc[Id][\"filepaths\"]\n    \n    #return the path\n    return path2\n\ndef get_image(Id,df,x=224,y=224):\n    \"\"\"Returns image of a given row of the birds dataframe\"\"\"\n    #get initial path to all data\n    path1 = \"kaggle/input/inputbirds/\"\n    \n    #call fet_path function\n    path2 = get_path(Id,df)\n    \n    #read image of the path1+path2 and return it\n    return Image.open(path1+path2).resize((x,y))\n\ndef get_input_NN(Id,df,gray = False):\n    \"\"\"Return a 3D matrix of image ready to be read in the CNN\"\"\"\n    \n    #call fet_image function to read image\n    im = get_image(Id,df)\n    \n    #transform iamge to RGB format\n    rgb_im = im.convert('RGB')\n    \n    if gray:\n        g_im = rgb_im.convert('L')\n        input_NN = asarray(g_im)\n                \n    else:\n    \n        #Empty zeros matrix to be filled with pixel values of the image\n        input_NN = np.zeros((3,224,224))\n\n        #fill up the matrix with RGB values at each point\n        for x in range(224):\n            for y in range(224):\n                r, g, b = rgb_im.getpixel((x,y))\n                input_NN[0,x,y] = r\n                input_NN[1,x,y] = g\n                input_NN[2,x,y] = b\n\n    #returns the 3D matrix a s a numpy array\n    return input_NN\n\n\ndef show_image(Id,inputs):\n    \"\"\"Print image from Id of image in the input array\"\"\"\n    im = plt.imshow(inputs[Id].astype('uint8'))\n\n\n\ndef get_class_dict():\n    \"\"\"Return dictionary of classes of each target\"\"\"\n\n    class_dict_df = pd.read_csv(\"/kaggle/input/inputbirds/class_dict.csv\")\n    class_dict = {}\n    for row in range(len(class_dict_df)):\n        class_dict[class_dict_df.iloc[row][1]] = class_dict_df.iloc[row][0]\n    return class_dict\n\n\ndef CNNDefinition(fil, k_size,  Poolsize, dense, \n                  input_shape, N_categ ,  loss = 'categorical_crossentropy',  \n                  metrics = ['accuracy'], last_act = \"softmax\", lmb = 0.01, eta = 0.001,summary = True):\n    \"\"\"\n    This CNNDefinition function allows the definition of a CNN network. The Network defined can be exploited for both \n    a regression and for a classification task.\n    \n    Parameters ---\n    fil : list of the number of filters per each convolutional layer,\n    k_size : dimension of the kernels related to each convolutional layer,\n    Poolsize : dimension of the pooling layer applied after each convolutional one,\n    dense : list of dimensions of dense layers applied after the convolutional step, \n    input_shape :  dimension of the input data , \n    N_categ : number of categories in the classification task (has to be set at 1 for regression),\n    loss = 'categorical_crossentropy',  \n    metrics = ['accuracy'], \n    last_act : default = \"sigmoid\", ok for classification. Has to be changed for regression\n    \"\"\"\n    model = keras.Sequential()    \n    model.add(layers.Conv2D(filters = fil[0], kernel_size = k_size[0],\n                            padding = \"same\",\n                            activation = \"relu\",\n                            input_shape = input_shape,\n                            kernel_regularizer=regularizers.l2(lmb)\n                            ))\n    model.add(layers.MaxPooling2D(Poolsize[0]))\n#     model.add(layers.SpatialDropout2D(0.3))\n\n    for i in range(1,len(fil)):\n        model.add(layers.Conv2D(filters = fil[i], kernel_size = k_size[i],\n                                padding = \"same\", activation = \"relu\",kernel_regularizer=regularizers.l2(lmb)))\n        model.add(layers.MaxPooling2D(Poolsize[i]))\n#         model.add(layers.SpatialDropout2D(0.3))\n\n    model.add(layers.Flatten())\n    for i in range(len(dense)):\n        model.add(layers.Dense(dense[i],activation=\"relu\",kernel_regularizer=regularizers.l2(lmb)))\n\n    #model.add(layers.Dropout(0.2))\n    model.add(layers.Dense(N_categ, activation= last_act))\n    \n    if summary:\n        print(model.summary())\n\n    #default optimizer is Adam, different learning rates affect the time the network takes to converge.\n    \n    #try different alogrithms and learning rates\n    optimizer = keras.optimizers.Adam()\n#     optimizer = keras.optimizers.SGD()\n    \n    optimizer.learning_rate.assign(eta)\n\n    model.compile(\n        optimizer = optimizer, \n        loss = loss, \n        metrics = metrics\n    )\n\n    return model\n\n\n\ndef Fit(model, X_train, Y_train, X_test, Y_test, pat = 100, mindelta = 0.01, batch = 64, epochs = 1200, verb = 1):\n    \n    '''\n    Fit function for fitting the model. Training and test data are specified as parameters; \n    EarlyStopping is introduced to avoid overfitting.\n    The function returns a dataframe containing the history of the training. The values of the accuracy and the different \n    epochs can be then inferred from such a returned quantity.\n    \n    Parameters ---\n    model : model to be fit,\n    X_train : train features,\n    Y_train : train targets,\n    X_test : test features,\n    Y_test : test features,\n    pat : default = 100, patience for the EarlyStopping, \n    mindelta : default = 0.01, mindelta in EarlyStopping, \n    batch : default = 64, batch size while fitting, \n    epochs : default = 1200, number of epochs for training,  \n    verb = 1):\n    '''\n\n    early_stopping = keras.callbacks.EarlyStopping(\n        patience = pat,\n        min_delta = mindelta,\n        restore_best_weights=True,\n    )\n\n    history = model.fit(\n        X_train, Y_train,\n        validation_data=(X_test, Y_test),\n        batch_size = batch,\n        epochs = epochs,\n        callbacks=[early_stopping],\n        verbose=verb, # hide the output because we have so many epochs\n    )\n    return pd.DataFrame(history.history)\n\n","metadata":{"execution":{"iopub.status.busy":"2022-02-02T16:37:33.537839Z","iopub.execute_input":"2022-02-02T16:37:33.538403Z","iopub.status.idle":"2022-02-02T16:37:33.561822Z","shell.execute_reply.started":"2022-02-02T16:37:33.538363Z","shell.execute_reply":"2022-02-02T16:37:33.560977Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"datagen_vgg = ImageDataGenerator() \n#\ntrain_generator_vgg = datagen_vgg.flow_from_directory('/kaggle/input/100-bird-species/train',\n                                                      batch_size=128,\n                                                      target_size=(224,224),\n                                                      class_mode='categorical')\n#\nvalidation_generator_vgg = datagen_vgg.flow_from_directory('/kaggle/input/100-bird-species/valid',\n                                                           batch_size=128,\n                                                           target_size=(224,224),\n                                                           class_mode='categorical')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# birds = pd.read_csv(\"/kaggle/input/inputbirds/birds.csv\")\n# label_list = list(birds.labels.unique())\n# sub_label_list = label_list[0:100]\n# birds_100 = birds[birds[\"labels\"].isin(sub_label_list)]\n\n# train_100 = birds_100[birds_100[\"data set\"] == \"train\"]\n\n# class_dict = get_class_dict()\n# # labels = np.zeros((2987))\n# labels = np.zeros((len(train_100)))\n# for label in range(len(train_100)):\n#     labels[label] = class_dict[train_100.iloc[label][1]]\n    \n# a_file = open(\"/kaggle/input/inputbirds/inputs_56_56_100.pkl\", \"rb\")\n# inputs = pickle.load(a_file)\n# inputs = inputs/255\n\n# train_size = 0.8\n# test_size = 1 - train_size\n# X_train, X_test, Y_train, Y_test = train_test_split(inputs, labels,  train_size=train_size,\n#                                                     test_size=test_size)\n\n# Y_train = to_categorical(Y_train)\n# Y_test = to_categorical(Y_test)\n# input_shape = inputs[0].shape","metadata":{"execution":{"iopub.status.busy":"2022-02-01T15:16:48.465503Z","iopub.execute_input":"2022-02-01T15:16:48.465766Z","iopub.status.idle":"2022-02-01T15:16:57.79992Z","shell.execute_reply.started":"2022-02-01T15:16:48.465738Z","shell.execute_reply":"2022-02-01T15:16:57.799032Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#code to read the train_20 birds subset with colors\n\nbirds = pd.read_csv(\"/kaggle/input/inputbirds/birds.csv\")\nlabel_list = list(birds.labels.unique())\nsub_label_list = label_list[0:20]\nbirds_20 = birds[birds[\"labels\"].isin(sub_label_list)]\n\ntrain_20 = birds_20[birds_20[\"data set\"] == \"train\"]\n\nclass_dict = get_class_dict()\nlabels = np.zeros((len(train_20)))\nfor label in range(len(train_20)):\n    labels[label] = class_dict[train_20.iloc[label][1]]\n    \na_file = open(\"/kaggle/input/inputbirds/inputs_56_56.pkl\", \"rb\")\ninputs = pickle.load(a_file)\ninputs = inputs/255\n\ntrain_size = 0.8\ntest_size = 1 - train_size\nX_train, X_test, Y_train, Y_test = train_test_split(inputs, labels,  train_size=train_size,\n                                                    test_size=test_size)\n\nY_train = to_categorical(Y_train)\nY_test = to_categorical(Y_test)\ninput_shape = inputs[0].shape","metadata":{"execution":{"iopub.status.busy":"2022-02-02T16:37:40.950673Z","iopub.execute_input":"2022-02-02T16:37:40.951426Z","iopub.status.idle":"2022-02-02T16:37:43.034844Z","shell.execute_reply.started":"2022-02-02T16:37:40.951375Z","shell.execute_reply":"2022-02-02T16:37:43.034106Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"#code to read the train_20 birds subset with gray scale\n\nbirds = pd.read_csv(\"/kaggle/input/inputbirds/birds.csv\")\nlabel_list = list(birds.labels.unique())\nsub_label_list = label_list[0:20]\nbirds_20 = birds[birds[\"labels\"].isin(sub_label_list)]\n\ntrain_20 = birds_20[birds_20[\"data set\"] == \"train\"]\n\nclass_dict = get_class_dict()\nlabels = np.zeros((len(train_20)))\nfor label in range(len(train_20)):\n    labels[label] = class_dict[train_20.iloc[label][1]]\n    \na_file = open(\"/kaggle/input/inputbirds/inputs_56_56_gray.pkl\", \"rb\")\ninputs = pickle.load(a_file)\ninputs = inputs/255\n\ninputs_corrected = np.zeros([len(train_20),56,56,1])\nfor n,image in enumerate(inputs):\n    inputs_corrected[n] = np.array([image]).T\n\n\ntrain_size = 0.8\ntest_size = 1 - train_size\nX_train_g, X_test_g, Y_train_g, Y_test_g = train_test_split(inputs_corrected, labels,  train_size=train_size,\n                                                    test_size=test_size)\n\nY_train_g = to_categorical(Y_train_g)\nY_test_g = to_categorical(Y_test_g)\ninput_shape_g = inputs_corrected[0].shape","metadata":{"execution":{"iopub.status.busy":"2022-02-02T12:08:08.621894Z","iopub.execute_input":"2022-02-02T12:08:08.622595Z","iopub.status.idle":"2022-02-02T12:08:09.117934Z","shell.execute_reply.started":"2022-02-02T12:08:08.62254Z","shell.execute_reply":"2022-02-02T12:08:09.117196Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"l_fil = [50,100] #number of filters randomly initialized -->\nl_ker = [2,4] #filters -->  size of the square filter\nl_pool = [5, 10]  #size of pooled matrix\nl_dense = []\nmodel_0 = CNNDefinition(l_fil, l_ker, l_pool, l_dense, X_train[0].shape, Y_train.shape[1], lmb = 0.00001, eta = 0.001)\nhistory_0 = Fit(model_0, X_train, Y_train, X_test, Y_test, epochs = 150)\n\nmodel_1 = CNNDefinition(l_fil, l_ker, l_pool, l_dense, X_train_g[0].shape, Y_train_g.shape[1], lmb = 0.00001, eta = 0.001)\nhistory_1 = Fit(model_1, X_train_g, Y_train_g, X_test_g, Y_test_g, epochs = 150)","metadata":{"execution":{"iopub.status.busy":"2022-02-02T12:08:23.207544Z","iopub.execute_input":"2022-02-02T12:08:23.207846Z","iopub.status.idle":"2022-02-02T12:09:46.135073Z","shell.execute_reply.started":"2022-02-02T12:08:23.207815Z","shell.execute_reply":"2022-02-02T12:09:46.134301Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(15,10))\nplt.plot(history_0.accuracy,\"--\", color = \"blue\", label=\"Train Accuracy color\")\nplt.plot(history_1.accuracy,\"--\", color =\"gray\",label=\"Train Accuracy gray\")\nplt.plot(history_0.val_accuracy, color = \"blue\",label=\"Test accuracy color\" )\nplt.plot(history_1.val_accuracy, color =\"gray\", label=\"Test accuracy gray\" )\nplt.legend(fontsize=20)\nplt.xlabel(\"Number of epochs\",fontsize=20)\nplt.ylabel(\"Accuracy\",fontsize=20)\nplt.savefig(\"graystudie.jpg\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-02-02T12:13:02.759765Z","iopub.execute_input":"2022-02-02T12:13:02.760015Z","iopub.status.idle":"2022-02-02T12:13:03.112735Z","shell.execute_reply.started":"2022-02-02T12:13:02.759985Z","shell.execute_reply":"2022-02-02T12:13:03.112101Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"l_fil = [50,100] #number of filters randomly initialized -->\nl_ker = [2,4] #filters -->  size of the square filter\nl_pool = [5, 10]  #size of pooled matrix\nl_dense = []\nmodel_0 = CNNDefinition(l_fil, l_ker, l_pool, l_dense, X_train[0].shape, Y_train.shape[1], lmb = 0.00001, eta = 0.001)\nhistory_0 = Fit(model_0, X_train, Y_train, X_test, Y_test, epochs = 150)","metadata":{"execution":{"iopub.status.busy":"2022-02-02T14:54:20.718407Z","iopub.execute_input":"2022-02-02T14:54:20.719076Z","iopub.status.idle":"2022-02-02T14:55:02.025478Z","shell.execute_reply.started":"2022-02-02T14:54:20.719029Z","shell.execute_reply":"2022-02-02T14:55:02.024715Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# CNN definition incuding droping\n\ndef CNNDefinition_(fil, k_size,  Poolsize, dense, \n                  input_shape, N_categ ,  loss = 'categorical_crossentropy',  \n                  metrics = ['accuracy'], last_act = \"softmax\", lmb = 0.01, eta = 0.001,summary = True,drop1 = 0.2):\n    \"\"\"\n    This CNNDefinition function allows the definition of a CNN network. The Network defined can be exploited for both \n    a regression and for a classification task.\n    \n    Parameters ---\n    fil : list of the number of filters per each convolutional layer,\n    k_size : dimension of the kernels related to each convolutional layer,\n    Poolsize : dimension of the pooling layer applied after each convolutional one,\n    dense : list of dimensions of dense layers applied after the convolutional step, \n    input_shape :  dimension of the input data , \n    N_categ : number of categories in the classification task (has to be set at 1 for regression),\n    loss = 'categorical_crossentropy',  \n    metrics = ['accuracy'], \n    last_act : default = \"sigmoid\", ok for classification. Has to be changed for regression\n    \"\"\"\n    model = keras.Sequential()    \n    model.add(layers.Conv2D(filters = fil[0], kernel_size = k_size[0],\n                            padding = \"same\",\n                            activation = \"relu\",\n                            input_shape = input_shape,\n                            kernel_regularizer=regularizers.l2(lmb)\n                            ))\n    model.add(layers.MaxPooling2D(Poolsize[0]))\n#     model.add(layers.SpatialDropout2D(drop1))\n\n    for i in range(1,len(fil)):\n        model.add(layers.Conv2D(filters = fil[i], kernel_size = k_size[i],\n                                padding = \"same\", activation = \"relu\",kernel_regularizer=regularizers.l2(lmb)))\n        model.add(layers.MaxPooling2D(Poolsize[i]))\n#         model.add(layers.SpatialDropout2D(drop))\n\n    model.add(layers.Flatten())\n    model.add(layers.Dropout(drop1))\n    for i in range(len(dense)):\n        model.add(layers.Dense(dense[i],activation=\"relu\",kernel_regularizer=regularizers.l2(lmb)))\n\n\n    model.add(layers.Dense(N_categ, activation= last_act))\n    \n    if summary:\n        print(model.summary())\n\n    #default optimizer is Adam, different learning rates affect the time the network takes to converge.\n    \n    #try different alogrithms and learning rates\n    optimizer = keras.optimizers.Adam()\n#     optimizer = keras.optimizers.SGD()\n    \n    optimizer.learning_rate.assign(eta)\n\n    model.compile(\n        optimizer = optimizer, \n        loss = loss, \n        metrics = metrics\n    )\n\n    return model\n\n","metadata":{"execution":{"iopub.status.busy":"2022-02-02T17:09:17.275722Z","iopub.execute_input":"2022-02-02T17:09:17.275977Z","iopub.status.idle":"2022-02-02T17:09:17.286995Z","shell.execute_reply.started":"2022-02-02T17:09:17.275947Z","shell.execute_reply":"2022-02-02T17:09:17.286325Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"print(\"hello\")","metadata":{"execution":{"iopub.status.busy":"2022-02-02T17:23:30.530115Z","iopub.execute_input":"2022-02-02T17:23:30.530599Z","iopub.status.idle":"2022-02-02T17:23:30.534941Z","shell.execute_reply.started":"2022-02-02T17:23:30.530563Z","shell.execute_reply":"2022-02-02T17:23:30.534087Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"drop = [0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9]\nl_fil = [50,100] #number of filters randomly initialized -->\nl_ker = [2,4] #filters -->  size of the square filter\nl_pool = [5, 10]  #size of pooled matrix\nl_dense = []\nlmb = 0.00001\neta = 0.001\n\ntrain_accuracy = np.zeros(10)\ntest_accuracy = np.zeros(10)\nn =  0\nfor d in tqdm(drop):\n    model = CNNDefinition_(l_fil, l_ker, l_pool, l_dense, X_train[0].shape, Y_train.shape[1], lmb = lmb, eta = eta, drop1 = d)\n    history = Fit(model, X_train, Y_train, X_test, Y_test, epochs = 500, verb = False)\n    test_accuracy[n] = max(history.val_accuracy)\n    train_accuracy[n] = max(history.accuracy)\n    print(max(history.accuracy), max(history.val_accuracy), d)\n    n+=1","metadata":{"execution":{"iopub.status.busy":"2022-02-02T17:28:52.730677Z","iopub.execute_input":"2022-02-02T17:28:52.731211Z","iopub.status.idle":"2022-02-02T17:40:20.001551Z","shell.execute_reply.started":"2022-02-02T17:28:52.731173Z","shell.execute_reply":"2022-02-02T17:40:20.000828Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(10,10))\nplt.plot(drop,test_accuracy,\"--\", label=\"Test\")\nplt.plot(drop,train_accuracy,\"--\", label=\"Train\")\nplt.legend(fontsize=20)\nplt.xlabel(\"Drop out\",fontsize=20)\nplt.ylabel(\"Accuracy\",fontsize=20)\nplt.savefig(\"drop_out_flattenlayer.jpg\")\nplt.show()\n\n","metadata":{"execution":{"iopub.status.busy":"2022-02-02T17:59:44.089769Z","iopub.execute_input":"2022-02-02T17:59:44.090497Z","iopub.status.idle":"2022-02-02T17:59:44.572578Z","shell.execute_reply.started":"2022-02-02T17:59:44.090457Z","shell.execute_reply":"2022-02-02T17:59:44.571897Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"history_1","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(15,10))\nplt.plot(history_0.accuracy,\"--\", label=\"DL Train Accuracy\")\nplt.plot(history_1.accuracy,\"--\", label=\"NDL Train Accuracy\")\nplt.plot(history_0.val_accuracy, label=\"DL Test accuracy\" )\nplt.plot(history_1.val_accuracy, label=\"NDL Test accuracy\" )\nplt.legend(fontsize=20)\nplt.xlabel(\"Number of epochs\",fontsize=20)\nplt.ylabel(\"Accuracy\",fontsize=20)\nplt.savefig(\"densevsNodense.jpg\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-02-01T16:24:03.553989Z","iopub.execute_input":"2022-02-01T16:24:03.554615Z","iopub.status.idle":"2022-02-01T16:24:03.946041Z","shell.execute_reply.started":"2022-02-01T16:24:03.554581Z","shell.execute_reply":"2022-02-01T16:24:03.945069Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Main cell code to produce grid searches, I just mdoify the grid parameters at each time, so there is not a code for eac plot obtained, but you can guess how to do it with the commented lines\n\n\nsns.set()\n\nl_fil = [50,100] #number of filters randomly initialized -->\nl_ker = [2,4] #filters -->  size of the square filter\nl_pool = [5, 10]  #size of pooled matrix\nl_dense = []\nlmb = 0.00001\neta = 0.001\n\ndrop1 = np.array([0.1,0.2,0.3,0.4,0.5])\ndrop2 = np.array([0.1,0.2,0.3,0.4,0.5])\n\n#train_accuracy = np.zeros((len(eta_vals), len(lmbd_vals)))\ntest_accuracy = np.zeros((len(drop1), len(drop2)))\n# test_accuracy = np.zeros((len(pool_size_1), len(pool_size_2)))\n\nfor i in tqdm(range(len(drop1))):\n    for j in tqdm(range(len(drop2))):\n        \n#         l_ker = [[filters_size_1[i], filters_size_1[i]],[filters_size_2[j], filters_size_2[j]]]\n#         l_pool = [[pool_size_1[i],pool_size_1[i]],[pool_size_2[j],pool_size_2[j]]]\n#         print(l_pool)\n        \n#         pool1 = pool_size_1[i]\n#         pool2 = pool_size_2[j]\n#         if int((int(56/pool1)/pool2)) == 0:\n#             test_accuracy[i][j] = 0\n            \n#         else:\n        print(drop1[i], drop2[j])\n        model = CNNDefinition_(l_fil, l_ker,l_pool, l_dense, X_train[0].shape, Y_train.shape[1],lmb = lmb, eta = eta, summary = False, drop1 = drop1[i], drop2 =drop2[j])\n        history = Fit(model, X_train, Y_train, X_test, Y_test, epochs = 100, verb = 0)\n        print(max(history.val_accuracy))\n        test_accuracy[i][j] = max(history.val_accuracy)\n\n        \n# fig, ax = plt.subplots(figsize = (10, 10))\n# sns.heatmap(train_accuracy, annot=True, ax=ax, cmap=\"viridis\")\n# ax.set_title(\"Training Accuracy\")\n# ax.set_ylabel(\"$\\eta$\")\n# ax.set_xlabel(\"$\\lambda$\")\n# plt.show()\n\nfig, ax = plt.subplots(figsize = (10, 10))\nsns.heatmap(test_accuracy, annot=True, ax=ax, cmap=\"viridis\",xticklabels=drop2, yticklabels=drop1)\nax.set_title(\"Test Accuracy\")\n# ax.set_xlabel(\"2nd pool size\")\n# ax.set_ylabel(\"1st pool size\")\nax.set_ylabel(\"drop after 1st pooling\")\nax.set_xlabel(\"drop after flatten layer\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-02-02T16:38:15.147528Z","iopub.execute_input":"2022-02-02T16:38:15.148089Z","iopub.status.idle":"2022-02-02T16:48:30.203892Z","shell.execute_reply.started":"2022-02-02T16:38:15.148048Z","shell.execute_reply":"2022-02-02T16:48:30.203199Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"fig, ax = plt.subplots(figsize = (10, 10))\nsns.heatmap(test_accuracy, annot=True, ax=ax, cmap=\"viridis\",xticklabels=drop2, yticklabels=drop1)\nax.set_title(\"Test Accuracy\",fontsize= 20)\n# ax.set_xlabel(\"2nd pool size\")\n# ax.set_ylabel(\"1st pool size\")\nax.set_ylabel(\"Spatial Drop out after 1st pooling\", fontsize = 20)\nax.set_xlabel(\"Drop out after flatten layer\",  fontsize = 20)\nplt.savefig(\"dropout_grid.jpg\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-02-02T17:00:59.186048Z","iopub.execute_input":"2022-02-02T17:00:59.186323Z","iopub.status.idle":"2022-02-02T17:00:59.576966Z","shell.execute_reply.started":"2022-02-02T17:00:59.186291Z","shell.execute_reply":"2022-02-02T17:00:59.576319Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#code for the gray comparison\n\nl_fil = [50,100] #number of filters randomly initialized -->\nl_ker = [2,4] #filters -->  size of the square filter\nl_pool = [5, 10]  #size of pooled matrix\nl_dense = []\nmodel = CNNDefinition(l_fil, l_ker, l_pool, l_dense, X_train[0].shape, Y_train.shape[1],lmb = 0.00001, eta = 0.001)\nhistory_gray = Fit(model, X_train, Y_train, X_test, Y_test, epochs = 150)\nplt.plot(history_gray.accuracy,\"--\")\nplt.plot(history_gray.val_accuracy,\"-\")","metadata":{"execution":{"iopub.status.busy":"2022-02-02T12:00:20.749753Z","iopub.execute_input":"2022-02-02T12:00:20.750015Z","iopub.status.idle":"2022-02-02T12:01:02.422633Z","shell.execute_reply.started":"2022-02-02T12:00:20.749985Z","shell.execute_reply":"2022-02-02T12:01:02.421951Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}